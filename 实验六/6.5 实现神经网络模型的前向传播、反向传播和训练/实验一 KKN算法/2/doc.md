
夏炘航
gold15480
6.5 实现神经网络模型的前向传播、反向传播和训练
实验总用时：00:00:01
nav
第2关：实现神经网络模型的反向传播
100
学习内容
参考答案
记录
评论
任务描述
相关知识
神经网络的反向传播
计算图
神经网络反向传播的实现
编程要求
测试说明
任务描述
本关任务：实现神经网络模型的反向传播。

相关知识
为了完成本关任务，你需要掌握：

神经网络反向传播的原理；
计算图上的反向传播；
神经网络反向传播的实现。
本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第 5 章的内容。

神经网络的反向传播
在之前的实训中，我们简单学习了神经网络的反向传播。神经网络的反向传播就是按照神经网络层的堆叠顺序的逆顺序，将后继网络层的输入的梯度作为前驱网络层反向传播的输入，通过网络层的反向传播运算规则，计算对应的输入的梯度和参数的梯度。在之前实训中，我们曾以一个简单的三层神经网络为例，这里我们简单做一个回顾。



图1
图1 简单三层神经网络
假设这个神经网络的三层分别为f 
1
​
 (x;W 
1
​
 ), f 
2
​
 (x;W 
2
​
 ), f 
3
​
 (x;W 
3
​
 )，每层之后的激活函数为g 
1
​
 (x), g 
2
​
 (x), g 
3
​
 (x)。网络训练使用的损失函数为L(x,t)，其中x表示网络的输出，t表示目标。那么这个网络的计算过程可以表示为：

y 
1
​
 
z 
1
​
 
y 
2
​
 
z 
2
​
 
y 
3
​
 
z 
3
​
 
​
  
=f 
1
​
 (x;W 
1
​
 )
=g 
1
​
 (y 
1
​
 )
=f 
2
​
 (z 
1
​
 ;W 
2
​
 )
=g 
2
​
 (y 
2
​
 )
=f 
3
​
 (z 
2
​
 ;W 
3
​
 )
=g 
3
​
 (y 
3
​
 )l=L(z 
3
​
 ,t)
​
 

之后，对第三层进行反向传播，按照相同的方法，可以对之前的网络层进行推导：

∂y 
3
​
 
∂l
​
 
∂W 
3
​
 
∂l
​
 
∂z 
2
​
 
∂l
​
 
​
  
= 
∂z 
3
​
 
∂l
​
 ⋅ 
∂y 
3
​
 
∂z 
3
​
 
​
 = 
∂z 
3
​
 
∂l
​
 ⋅ 
∂y 
3
​
 
∂g 
3
​
 (y 
3
​
 )
​
 
= 
∂y 
3
​
 
∂l
​
 ⋅ 
∂W 
3
​
 
∂y 
3
​
 
​
 = 
∂y 
3
​
 
∂l
​
 ⋅ 
∂W 
3
​
 
∂f 
3
​
 (z 
2
​
 ;W 
3
​
 )
​
 
= 
∂y 
3
​
 
∂l
​
 ⋅ 
∂z 
2
​
 
∂y 
3
​
 
​
 = 
∂y 
3
​
 
∂l
​
 ⋅ 
∂z 
2
​
 
∂f 
3
​
 (z 
2
​
 ;W 
3
​
 )
​
 
​
 

计算图
这里我们希望进一步引入计算图的反向传播。在上一关中，我们学习了神经网络的计算图把每个网络层拆解成一系列的元操作，这些元操作对应计算节点，所有的中间结果对应数据节点，这些节点按照计算顺序形成一个 DAG 的结构。

在反向传播时，对于有多个输入的层，对于每个输入的反向传播可能会不同。一个典型的例子就是矩阵乘法算子 y=x 
T
 W，通过前面我们对全连接层的学习，我们知道对 x 和对 W 的反向传播计算是不同的，此时在计算图中矩阵乘法算子的反向传播就需要拆成两个算子，变成两个计算节点。下图展示了一个使用 sigmoid 激活函数的全连接层对应的前向和反向传播的计算图。



图1
图2 使用sigmoid激活函数的全连接层对应的前向和反向传播的计算图
神经网络反向传播的实现
实训拓展了在之前的实训定义的TinyNet类，实训已经给出了forward(x, t)的实现，并针对反向传播的需要对其进行了一定的修改。你需要实现该类的反向传播函数backward()。你需要将构造函数中的W_conv1、b_conv1、W_conv2、b_conv2、W_fc、b_fc的梯度按照顺序返回。

编程要求
根据提示，在右侧编辑器中 Begin 和 End 之间补充代码，实现上述 TinyNet 的定义和前向传播。

测试说明
平台会对你编写的代码进行测试，测试方法为：
平台会随机产生输入x、目标t以及三组权重和偏置，然后根据你的实现代码，创建一个TinyNet类的实例，然后利用该实例进行前向传播计算，再进行反向传播的计算。你的答案将与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过 1e-5 即可。

开始你的任务吧，祝你成功！

说点什么
resize-icon
1234567891011121314151617181920
import numpy
from layers import Convolution, Relu, FullyConnected, MaxPool, SoftmaxWithLoss


class TinyNet:
    def __init__(self, W_conv1, b_conv1, W_conv2, b_conv2, W_fc, b_fc):
        self.conv1 = Convolution(W_conv1, b_conv1, stride=1, pad=1)
        self.relu1 = Relu()

测试结果
测试集1
本关最大执行时间：100秒
上一关
下一关
run
评测
