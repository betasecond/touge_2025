
夏炘航
gold15480
6.5 实现神经网络模型的前向传播、反向传播和训练
实验总用时：00:00:00
nav
第3关：实现神经网络的梯度下降训练
100
学习内容
参考答案
记录
评论
任务描述
相关知识
神经网络的训练
随机梯度下降
欠拟合与过拟合
随机梯度下降的实现
编程要求
测试说明
任务描述
本关任务：实现神经网络的梯度下降训练。

相关知识
为了完成本关任务，你需要掌握：梯度下降训练的原理。

本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第 6 章的内容。

神经网络的训练
神经网络是一类非常典型的非凸模型，对与非凸函数进行优化的问题是非凸优化问题，而解决非凸优化问题的最常用的方法就是梯度下降。在之前的实训中，我们学习过梯度和梯度下降法的概念，这里做一个简单的回顾。梯度是函数值上升最快的参数变化方向，通常来说，这也是函数值下降最快的参数变化方向的负方向。如果我们能够求得每个参数的梯度∂l/∂w，那么我们就可以令所有的参数沿着其负梯度方向前进一小步，得到一组新的参数。这就是梯度下降法的基本思想，这一小步的距离叫做学习率η。参数更新的过程可以用下面公式表示：

w 
i
′
​
 =w 
i
​
 −η⋅ 
∂w 
i
​
 
∂l
​
 

如果这一过程延续足够长的时间，我们就可以期望模型能够收敛到一个足够好的位置（局部最优）。对于非凸优化问题，我们通常不期望模型能够收敛到全局最优，而只是期望模型能够收敛到一个足够好的局部最优。这个过程可以用下图表示：



图1
图1 梯度下降法
随机梯度下降
神经网络模型的训练离不开数据。在之前的实训中，我们可以看到，损失函数值的计算只与当前的 batch 有关。在使用梯度下降时，一种可行的方法是对于所有的训练数据，计算损失函数值，进而计算梯度，更新权重。但是，这样存在一个问题，就是每次更新需要的计算量非常大。目前，用来训练神经网络的数据集非常巨大，对整个数据集计算损失再进行更新效率非常低，因此，我们引入随机梯度下降。随机梯度下降的思想是，每次从训练数据中随机取出若干个，构成一个 batch，每次只对这一个 batch 计算损失和计算梯度，进而更新权重。数学上可以证明，随机梯度下降也可以保证网络的收敛。

通常，随机梯度下降在采样训练数据时并不是完全随机采样的，而是先将整个数据集随机排序，然后从头开始依次取。按照这样的方式将整个数据集里的数据全都选取了一遍叫做一个 epoch，每次取的叫一个 batch 或者一个 iteration。

欠拟合与过拟合
机器学习模型在训练时还有另外一个重要的问题，那就是欠拟合（underfit）与过拟合（overfit）。在本质上这是一个数据集与模型拟合能力相匹配的问题。模型的参数越多，模型越复杂，那么模型拟合数据的能力就越强。但是，如果数据比较简单，那么用一个过分强大的模型来拟合这个数据集会造成模型“记住”了每个训练样本，而不是从训练数据中挖掘出共性，从而造成过拟合；而如果数据非常复杂，而我们使用了一个过分简单的模型，那么模型就难以挖掘到数据背后的模式，从而造成欠拟合。下图展示了欠拟合与过拟合。图中的样本是从一个二次曲线上采样下来的，如果我们用一个线性函数来拟合，那么会造成欠拟合；而如果我们用一个高次函数来拟合，就会造成过拟合。这个高次函数经过了所有的样本点，但明显这不是我们想要的那个。



图1
图2 欠拟合、过拟合和拟合
那么欠拟合与过拟合要怎么解决呢？对于欠拟合，我们通常采用的方法是设计更复杂、拟合能力更强的神经网络；而对于过拟合，我们通常采用的方法是正则化（regularization）。而通常采用的正则化方法就是 L2 正则化。L2 正则化的基本思想是在 loss 中加入一个正则化项，这个正则化项是模型中的每个参数的 2 范数，即：

L=L(x,t)+λ⋅∑ 
2
1
​
 w 
i
2
​
 

通过最小化这个总损失，可以使得每个参数尽量小，从而抑制过拟合。λ是正则化系数，通常称为 weight decay，常用值为 1e-5。值得注意的是，正则化项只与参数本身有关，与模型的输入以及样本的标签都没有关系，因此，正则化项不需要显式的放在损失函数中计算，而是可以在更新参数的时候直接加到参数对应的梯度中。

随机梯度下降的实现
在本实训，你将对之前定义的 TinyNet，实现一次随机梯度下降的迭代。具体来说，你要实现train_one_iter函数，该函数接受 9 个参数：TinyNet 的三组权重和偏置、这个 iteration 的输入数据x、标签t、学习率learning_rate和正则化系数weight_decay。在该函数中，你要先构建一个TinyNet实例，然后先进行前向传播，再进行反向传播，最后对模型参数进行更新，最后把更新后的参数按照输入顺序返回。

编程要求
根据提示，在右侧编辑器 Begin 和 End 之间补充代码，实现随机梯度下降的训练。

测试说明
平台会对你编写的代码进行测试，测试方法为：
平台会随机产生输入x、目标t以及三组权重和偏置，并制定学习率和正则化系数，然后根据你的实现调用train_one_iter函数。你的答案将与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过 1e-5 即可。

开始你的任务吧，祝你成功！

说点什么
resize-icon
1234567891011121314151617181920
import numpy
from layers import Convolution, Relu, FullyConnected, MaxPool, SoftmaxWithLoss


class TinyNet:
    def __init__(self, W_conv1, b_conv1, W_conv2, b_conv2, W_fc, b_fc):
        self.conv1 = Convolution(W_conv1, b_conv1, stride=1, pad=1)
        self.relu1 = Relu()

测试结果
测试集1
本关最大执行时间：100秒
上一关
run
评测
