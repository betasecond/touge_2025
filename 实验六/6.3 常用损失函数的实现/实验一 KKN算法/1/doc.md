
夏炘航
gold15480
6.3 常用损失函数的实现
实验总用时：00:00:26
nav
第1关：实现常见损失函数的前向传播
100
学习内容
参考答案
记录
评论
任务描述
相关知识
神经网络的训练
常见损失函数的定义
常见损失函数前向传播的实现
编程要求
测试说明
任务描述
本关任务：实现常见损失函数的前向传播。

相关知识
为了完成本关任务，你需要掌握：常见损失函数的定义。

本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第4.1−4.2章节的内容。

神经网络的训练
神经网络作为上世纪90年代就已经出现的技术，为什么忽然在2012年以后重新进入人们的视野，并在计算机视觉、自然语言处理领域等领域展现了前所未有的统治力呢？答案包括两个因素，第一是计算机硬件的高速发展，为深度学习提供了强大的算力支撑；第二就是数据，随着云计算等技术的高速发展，互联网为深度学习提供了海量的数据。可以说，数据是深度学习的命脉，深度学习是由数据驱动的，数据质量也是决定神经网络模型性能的最关键的因素。

那么有了数据，怎么来获得一个能够解决问题的模型呢？这个过程就是模型的训练。训练的目的是使模型识别出数据存在的模式（pattern），可以理解为数据背后隐含的有一定共性的特征。在训练模型时，我们需要设定一个目标，使得模型的输出向着我们希望的结果靠近。用来衡量模型的输出与我们期望结果的接近程度的就是损失函数。在不同类型的任务中，我们会使用不同的损失函数。下面，我们对常用的损失函数进行介绍。

常见损失函数的定义
1. Cross Entropy

在分类任务中，网络通常会对输入样本属于每个类的概率进行预测，而我们的目标则是期望正确的类的概率最大。这就是最大似然概率的思想。基于这个思想，我们可以得到交叉熵（Cross Entropy）损失函数。其函数表达式为：

E=−∑ 
i=1
C
​
 q 
i
​
 log(p 
i
​
 )

其中q 
i
​
 是标签类别的 one-hot 编码，当样本属于第i个类别时，q 
i
​
 =1，否则q 
i
​
 =0；C表示类别的个数；p 
i
​
 是预测的每个类别的概率，通常是 softmax 的输出：

p 
i
​
 =e 
x 
i
​
 
 /∑ 
j=1
C
​
 e 
x 
j
​
 
 

其中x 
i
​
 是网络模型的输出。softmax 可以将网络的输出转化为和为1的若干个正实数，x 
i
​
 越大对应的p 
i
​
 越大，因此可以看做是多个类别的概率。

在实现交叉熵时，为了反向传播时求导的简单，通常与 softmax 一起实现，即对于模型的输出，先做一次 softmax，再计算交叉熵损失。这种做法对应的反向传播的优点会在下一关中介绍。

2. Mean Squared Error

回归问题相比于分类问题要更加简单。在回归问题中，我们的目标是使得网络模型的输出与目标尽可能的接近。为了实现这一目标，最直接的想法就是使用均方误差（Mean Squared Error）。均方误差是回归问题中常用的损失函数，其核心思想是使得模型的预测结果与目标的差的平方最小。其函数表达式为：

E=0.5∑ 
i=1
N
​
 (y 
i
​
 −t 
i
​
 ) 
2
 

其中y 
i
​
 是网络模型的输出，t 
i
​
 是我们期望的目标，N是输出的个数。

常见损失函数前向传播的实现
对于交叉熵损失函数，实训已经预先定义了一个SoftmaxWithLoss类，该类是 softmax 和交叉熵的复合实现。你需要实现该类的前向传播函数forward(x, t)。forward(x, t)函数的输入x是一个维度等于2的numpy.ndarray，形状为(B,C)，其中B是 batch size，C是类别的个数；t是 batch 中每个样本所属的类别，是一个形状为(B,)的int类型的numpy.ndarray。首先，你需要对x沿着第二个维度进行 softmax 操作。实训已经提供了一个 softmax 函数的实现，你可以直接使用。之后，你需要实现交叉熵损失函数。最后的输出是 batch 中所有样本的损失的平均值。

对于均方误差损失函数，实训已经预先定义了一个MeanSquaredError类。你需要实现该类的前向传播函数forward(y, t)。forward()函数的输入y是一个维度等于2的numpy.ndarray，形状为(B,N)，其中B是 batch size，N是输出的个数；t也是一个形状为(B,N)的numpy.ndarray，代表每个输出的期望值。你需要实现均方误差损失函数，最后的输出是 batch 中所有样本的损失的和。

对于上面两个损失函数，你都需要将最后的损失值记录在self.loss中。

编程要求
根据提示，在右侧编辑器 Begin 和 End 之间补充代码，实现上述损失函数。

测试说明
平台会对你编写的代码进行测试，测试方法为：平台会随机产生输入x/y和目标t，然后根据你的实现代码，创建一个SoftmaxWithLoss/MeanSquaredError类的实例，然后利用该实例进行前向传播计算。你的答案将并与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过10 
−5
 即可。

样例输入：

# 对于SoftmaxWithLoss损失函数：
x:
[[-1  0  1]
 [-2  0  2]]
t:
[1, 2]
# 输出loss
0.775
# 对于MeanSquaredError损失函数：
y:
[[-1  0  1]
 [-2  0  2]]
t:
[[0  0  0]
 [0  0  0]]
# 输出loss
5.0
上述结果有四舍五入的误差，你可以忽略。

开始你的任务吧，祝你成功！

说点什么
resize-icon
12345678910111213141516171819202122
import numpy as np


def softmax(x):
    x = x - np.max(x, axis=1, keepdims=True)
    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)


class SoftmaxWithLoss:
    def __init__(self):

测试结果
测试集1
本关最大执行时间：20秒
下一关
run
评测
