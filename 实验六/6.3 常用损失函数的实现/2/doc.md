
夏炘航
gold15880
6.3 常用损失函数的实现
实验总用时：00:00:01
nav
第2关：实现常见损失函数的反向传播
100
学习内容
参考答案
记录
评论
任务描述
相关知识
梯度下降法
常见损失函数的反向传播
常见损失函数前向传播的实现
编程要求
测试说明
任务描述
本关任务：实现常见损失函数的反向传播。

相关知识
为了完成本关任务，你需要掌握：常见损失函数的定义。

本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第4.1−4.2章节的内容。

梯度下降法
通过上一关的学习，我们知道损失函数的作用是衡量模型的输出与我们的期望值之间的差距。那么，损失函数值越小，也就代表着我们更有可能获得一个具有更好性能的模型。那么，怎么改变模型中参数的值，才能令损失函数值不断变小呢？这里我们引入梯度的概念。梯度是函数值上升最快的参数变化方向，通常来说，这也是函数值下降最快的参数变化方向的负方向。如果我们能够求得每个参数的梯度∂l/∂w，那么我们就可以令所有的参数沿着其负梯度方向前进一小步，得到一组新的参数。这就是梯度下降法的基本思想，这一小步的距离叫做学习率η。参数更新的过程可以用下面公式表示：

w 
i
′
​
 =w 
i
​
 −η⋅ 
∂w 
i
​
 
∂l
​
 

那么，现在问题来了，怎样才能对每个参数求出它们的梯度呢？这一过程称为神经网络的反向传播（backprop）。对于神经网络来说，因为其堆叠的结构，使得求梯度这件事变成稍微有些复杂。但也因为这种堆叠结构，使得这个过程也可以根据堆叠的顺序一步一步求解。反向传播的核心思想是求导的链式法则，即：

∂x
∂l
​
 = 
∂f(x)
∂l
​
 ⋅ 
∂x
∂f(x)
​
 

这样，神经网络的求梯度过程就变成了对每一层求梯度的过程。在本实训中，我们聚焦在上面公式的第一项，即损失函数自身的反向传播。

常见损失函数的反向传播
1. Cross Entropy

在上一关中，我们介绍了交叉熵损失函数的函数表达式为：

E=−∑ 
i=1
C
​
 t 
i
​
 log(y 
i
​
 )

如果合并 softmax，那么则有：

y 
i
​
 
E
​
  
= 
∑ 
j=1
C
​
 e 
x 
i
​
 
 
e 
x 
i
​
 
 
​
 
=− 
i=1
∑
C
​
 t 
i
​
 log(y 
i
​
 )
​
 

上面公式对应的x 
i
​
 的梯度的计算为：

∂x 
i
​
 
∂l
​
 =y 
i
​
 −t 
i
​
 

当t为 one-hot 编码时，则可以进一步简化为：

∂x 
k
​
 
∂l
​
 =y 
k
​
 −1

其中k是正确类别对应的编号。上面公式的推导过程略微繁琐，感兴趣的同学可以参考这篇文章。

2. Mean Squared Error

相比于交叉熵，均方误差的反向传播较为简单。回顾均方误差的前向传播公式：

E=0.5∑ 
i=1
N
​
 (y 
i
​
 −t 
i
​
 ) 
2
 

我们可以直接写出其反向传播的公式：

∂y 
i
​
 
∂l
​
 =y 
i
​
 −t 
i
​
 

其中y 
i
​
 是网络模型的输出，t 
i
​
 是我们期望的目标，N是输出的个数。

常见损失函数前向传播的实现
对于交叉熵损失函数，实训拓展了上一关中定义的SoftmaxWithLoss类，该类是 softmax 和交叉熵的复合实现。实训已经给出了forward(x, t)的实现，并针对反向传播的需要对其进行了一定的修改。你需要实现该类的反向传播函数backward()。backward()函数不需要任何的输入，你需要根据forward(x, t)函数调用时记录的self.t和self.y的值，来计算forward(x, t)输入x的梯度，并作为backward()的返回值返回。

对于均方误差损失函数，实训拓展了上一关中定义的MeanSquaredError类，该类是 softmax 和交叉熵的复合实现。实训已经给出了forward(y, t)的实现，并针对反向传播的需要对其进行了一定的修改。你需要实现该类的反向传播函数backward()。backward()函数不需要任何的输入，你需要根据forward(y, t)函数调用时记录的self.t和self.y的值，来计算forward(y, t)输入y的梯度，并作为backward()的返回值返回。

编程要求
根据提示，在右侧编辑器 Begin 和 End 之间补充代码，实现上述损失函数。

测试说明
平台会对你编写的代码进行测试，测试方法为：平台会随机产生输入x/y和目标t，然后根据你的实现代码，创建一个SoftmaxWithLoss/MeanSquaredError类的实例，然后利用该实例先进行前向传播计算，在进行反向传播计算。你的答案将并与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过10 
−5
 即可。

样例输入：

# 对于SoftmaxWithLoss损失函数：
x:
[[-1  0  1]
 [-2  0  2]]
t:
[1, 2]
# x的梯度
[[ 0.04501529 -0.37763578  0.33262047]
 [ 0.00793812  0.05865521 -0.06659332]]
# 对于MeanSquaredError损失函数：
y:
[[-1  0  1]
 [-2  0  2]]
t:
[[0  0  0]
 [0  0  0]]
# y的梯度
[[-1  0  1]
 [-2  0  2]]
上述结果有四舍五入的误差，你可以忽略。

开始你的任务吧，祝你成功！

说点什么
resize-icon
12345678910111213141516171819202122
import numpy as np


def softmax(x):
    x = x - np.max(x, axis=1, keepdims=True)
    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)


class SoftmaxWithLoss:
    def __init__(self):

测试结果
测试集1
本关最大执行时间：20秒
上一关
run
评测
