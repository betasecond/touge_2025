
夏炘航
gold15480
6.4 全连接层和激活函数的反向传播的实现
实验总用时：00:00:20
nav
第1关：实现全连接层的反向传播
100
学习内容
参考答案
记录
评论
任务描述
相关知识
神经网络的反向传播
全连接层的反向传播
全连接层反向传播的实现
编程要求
测试说明
任务描述
本关任务：实现全连接层的反向传播。

相关知识
为了完成本关任务，你需要掌握：

神经网络的反向传播；
全连接层的反向传播。
本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第5章的内容。

神经网络的反向传播
在之前的实训中，我们学习了神经网络通过反向传播来计算每个参数的梯度，同时反向传播的核心思想是求导的链式法则，即：

∂x
∂l
​
 = 
∂f(x)
∂l
​
 ⋅ 
∂x
∂f(x)
​
 

那么，给定一个神经网络，反向传播是如何进行的呢？这里我们以一个三层神经网络为例，来讲解神经网络的反向传播。下图展示了这个简单神经网络的结构。



图1
图1 简单三层神经网络
首先，我们引入一个记号f(x;W)，来表示输入为x、参数为W的一个网络层。假设这个神经网络的三层分别为f 
1
​
 (x;W 
1
​
 ), f 
2
​
 (x;W 
2
​
 ), f 
3
​
 (x;W 
3
​
 )，每层之后的激活函数为g 
1
​
 (x), g 
2
​
 (x), g 
3
​
 (x)。网络训练使用的损失函数为L(x,t)，其中x表示网络的输出，t表示目标。那么这个网络的计算过程可以表示为：

y 
1
​
 
z 
1
​
 
y 
2
​
 
z 
2
​
 
y 
3
​
 
z 
3
​
 
l
​
  
=f 
1
​
 (x;W 
1
​
 )
=g 
1
​
 (y 
1
​
 )
=f 
2
​
 (z 
1
​
 ;W 
2
​
 )
=g 
2
​
 (y 
2
​
 )
=f 
3
​
 (z 
2
​
 ;W 
3
​
 )
=g 
3
​
 (y 
3
​
 )
=L(z 
3
​
 ,t)
​
 

在进行反向传播时，首先我们对损失函数进行反向传播：

∂z 
3
​
 
∂l
​
 = 
∂z 
3
​
 
∂L(z 
3
​
 ,t)
​
 

之后，对第三层进行反向传播，按照相同的方法，可以对之前的网络层进行推导：

∂y 
3
​
 
∂l
​
 
∂W 
3
​
 
∂l
​
 
∂z 
2
​
 
∂l
​
 
​
  
= 
∂z 
3
​
 
∂l
​
 ⋅ 
∂y 
3
​
 
∂z 
3
​
 
​
 = 
∂z 
3
​
 
∂l
​
 ⋅ 
∂y 
3
​
 
∂g 
3
​
 (y 
3
​
 )
​
 
= 
∂y 
3
​
 
∂l
​
 ⋅ 
∂W 
3
​
 
∂y 
3
​
 
​
 = 
∂y 
3
​
 
∂l
​
 ⋅ 
∂W 
3
​
 
∂f 
3
​
 (z 
2
​
 ;W 
3
​
 )
​
 
= 
∂y 
3
​
 
∂l
​
 ⋅ 
∂z 
2
​
 
∂y 
3
​
 
​
 = 
∂y 
3
​
 
∂l
​
 ⋅ 
∂z 
2
​
 
∂f 
3
​
 (z 
2
​
 ;W 
3
​
 )
​
 
​
 

通过上面的推导可以看到，对于一个特定的网络层，只要其输出的梯度已知，那么其参数和输入的梯度只与该层的计算有关，而与之前和之后的网络层是什么没有关系。因此，只要对每个网络层都定义好其前向传播和反向传播的计算，那么神经网络就可以计算每个参数的梯度。在这个过程中，神经网络的计算形成了计算图，这里不做详细展开，感兴趣的同学可以参考教材第5.1−5.2章节的内容。

全连接层的反向传播
下面，我们来学习全连接层的反向传播。首先，回顾一下全连接层的前向传播。一个包含N个输入神经元，M个输出神经元的全连接层包含两组参数：权重W∈R 
N×M
 和偏置b∈R 
M
 ，其输入可以看作是一个N维的（列）向量x∈R 
N
 ，此时全连接层的前向传播的计算可以表示为：

y=x 
T
 W+b

先假设根据链式法则，已知∂l/∂y，按照全连接层的定义，其应该是一个M维的（列）向量。根据矩阵计算求导法则，可以得到：

∂b
∂l
​
 
∂W
∂l
​
 
∂x
∂l
​
 
​
  
= 
∂y
∂l
​
 ⋅ 
∂b
∂y
​
 = 
∂y
∂l
​
 
= 
∂y
∂l
​
 ⋅ 
∂W
∂y
​
 =x×( 
∂y
∂l
​
 ) 
T
 
= 
∂y
∂l
​
 ⋅ 
∂x
∂y
​
 =W× 
∂y
∂l
​
 
​
 

全连接层反向传播的实现
实训拓展了在之前的实训定义的FullyConnected类，实训已经给出了forward(x)的实现，并针对反向传播的需要对其进行了一定的修改。你需要实现该类的反向传播函数backward(dout)，dout是损失函数相对于全连接层输出的梯度，即之前公式中的 
∂y
∂l
​
 ，是一个形状为(B,M)的numpy.ndarray，M是全连接层的输出通道数。在前向传播时，因为全连接层的输入的形状可以有所变化，并记录在了self.original_x_shape中，因此在计算完成后，请把输入的梯度还原为原来的形状。全连接层的输入记录在了self.x中。在反向传播的过程中，请将self.W和self.b的梯度分别存储在self.dW和self.db中，并将x的梯度返回。

编程要求
根据提示，在右侧编辑器 Begin 和 End 之间补充代码，实现全连接层的反向传播。

测试说明
平台会对你编写的代码进行测试，测试方法为：平台会随机产生输入x、权重W、偏置b和输出梯度dout，然后根据你的实现代码，创建一个FullyConnected类的实例，然后利用该实例先进行前向传播计算，再进行反向传播计算。你的答案将并与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过10 
−5
 即可。

样例输入：

W:
[[0.1, 0.2, 0.3],
[0.4, 0.5, 0.6]]
b:
[0.1, 0.2, 0.3]
x:
[[1, 2],
[3, 4]]
dout:
[[0.1, 0.2, 0.3],
[0.4, 0.5, 0.6]]
则对应的梯度为：

dx:
[[0.14 0.32]
 [0.32 0.77]]
dW:
[[1.3 1.7 2.1]
 [1.8 2.4 3. ]]
db:
[0.5 0.7 0.9]
上述结果有四舍五入的误差，你可以忽略。

开始你的任务吧，祝你成功！

说点什么
resize-icon
12345678910111213141516171819202122
import numpy as np


class FullyConnected:
    def __init__(self, W, b):
        r'''
        全连接层的初始化。

        Parameter:
        - W: numpy.array, (D_in, D_out)

测试结果
测试集1
本关最大执行时间：20秒
下一关
run
评测
