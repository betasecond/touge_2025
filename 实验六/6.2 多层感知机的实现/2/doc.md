
夏炘航
gold15480
6.2 多层感知机的实现
实验总用时：00:00:01
nav
第2关：实现常见激活函数的前向传播
100
学习内容
参考答案
记录
评论
任务描述
相关知识
激活函数的作用
常见激活函数的定义
常见激活函数的实现
编程要求
测试说明
任务描述
本关任务：实现常见激活函数的前向传播。

相关知识
为了完成本关任务，你需要掌握：常见激活函数的定义。

本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第 3.1-3.5 章节的内容。

激活函数的作用
我们首先来考虑这样一种神经网络，它由两个全连接层组成。假设这两个全连接层的权重和偏置分别为W 
1
​
 、b 
1
​
 和W 
2
​
 、b 
2
​
 ，输入为x，那么由上一个关中全连接层的计算公式，我们可以得到一些结论：

y
​
  
=(x×W 
1
​
 +b 
1
​
 )×W 
2
​
 +b 
2
​
 
=x×W 
1
​
 ×W 
2
​
 +b 
1
​
 ×W 
2
​
 +b 
2
​
 
​
 

因为矩阵乘法满足结合律，令W 
0
​
 =W 
1
​
 ×W 
2
​
 ，b 
0
​
 =b 
1
​
 ×W 
2
​
 +b 
2
​
 ，那么上面的两层全连接层就可以写为：

y=x×W 
0
​
 +b 
0
​
 

这就说明这样堆叠两个全连接层与一个全连接层是等效的，那么这种堆叠就是无效的，并不能增强模型的拟合能力。为了解决这一问题，通常在每一层的后面接一个非线性激活函数。通过引入非线性激活函数，可以极大的增强模型的拟合能力。

回顾之前我们使用感知机实现逻辑门的时候，我们在线性变换之后使用了符号函数，其实就是一种激活函数。也正是因为使用了符号函数，使得我们能够通过将感知机堆叠成多层感知机的方式实现异或门。

常见激活函数的定义
1. sigmoid激活函数

sigmoid 激活函数是一个 S 型函数，可以将任意范围的输入转化到[0,1]的范围内，而这一范围与概率的范围是相同的，因此可以借此实现“概率”。另一方面，sigmoid 激活函数在x的绝对值较大时，梯度非常接近0，会导致“梯度消失”的问题，使得网络难以收敛。其函数表达式如下：

sigmoid(x)=1/(1+e 
−x
 )

目前，sigmoid 激活函数的应用场景主要包含两种：第一是用于计算概率，这与 sigmoid 函数的本身性质吻合；第二是用于计算注意力，注意力是在计算机视觉和自然语言处理领域常用的技术，这留给感兴趣的学员自行学习。

2. ReLU激活函数

ReLU 激活函数是目前深度学习中最常用的激活函数，其只保留输入张量大于0的部分，而将小于0的部分置为0。与之前的 sigmoid 不同，ReLU 激活函数不会受到梯度消失的影响。其函数表达式如下：

ReLU(x)=max(0,x)

ReLU 是目前卷积神经网络中应用最多的激活函数，其特点是计算简单，同时不会受到梯度消失问题的影响。但是 ReLU 也存在着一定的问题，例如 ReLU 的输出都是非负数，因此 ReLU 输出的平均值一定是个正数，而不是 0。这可能会对网络的训练产生一定的影响。针对这一问题，研究者们也提出了 LReLU、PReLU 等方法进行改进，有兴趣的学员可以查找相关资料进行学习。

常见激活函数的实现
本实训希望你实现 sigmoid 和 ReLU 激活函数的前向传播。实训已经预先定义了一个Sigmoid类和一个ReLU类。在本实训中，你需要实现前向传播函数forward()。forward()函数的输入x是一个维度大于等于2的numpy.ndarray，形状为(B,d 
1
​
 ,d 
2
​
 ,...,d 
k
​
 )，其中B是batch size。返回经过激活函数处理的输出值。

编程要求
根据提示，在右侧编辑器中 Begin 和 End 之间补充代码，实现上述激活函数。

为了以后实现反向传播的方便，这里希望你：

在实现 sigmoid 时，将输出结果记录在self.out中；
在实现 ReLU 时，将小于 0 的元素按照 mask 的形式记录在self.mask中。
测试说明
平台会对你编写的代码进行测试，测试方法为：平台会随机产生输入x和输出梯度dout，然后根据你的实现代码，创建一个Sigmoid/ReLU类的实例，然后利用该实例进行前向传播计算。你的答案将并与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过1e-5即可。

样例输入：

# 对于sigmoid激活函数：
x:
[[-1, 0, 1]]
#对于ReLU激活函数：
x:
[[-1, 0, 1]]
则对应的输出为：

# 对于sigmoid激活函数：
[[0.27, 0.50, 0.73]]
#对于ReLU激活函数：
[[0, 0, 1]]
上述结果有四舍五入的误差，你可以忽略。

开始你的任务吧，祝你成功！

说点什么
resize-icon
12345678910111213141516171819202122
import numpy as np


class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        r'''
        Sigmoid激活函数的前向传播。

测试结果
测试集1
本关最大执行时间：20秒
上一关
run
评测
