
夏炘航
gold15480
6.2 多层感知机的实现
实验总用时：00:00:05
nav
第1关：实现全连接层的前向传播
100
学习内容
参考答案
记录
评论
任务描述
相关知识
神经网络的结构
全连接层的定义
全连接层的实现
编程要求
测试说明
任务描述
本关任务：实现全连接层的前向传播。

相关知识
为了完成本关任务，你需要掌握：

神经网络的结构；
全连接层的定义。
本实训内容可参考《深度学习入门——基于 Python 的理论与实现》一书中第 3.1-3.5 章节的内容。

神经网络的结构
在上一个实训中，我们对感知机和多层感知机进行了学习，也了解了利用感知机的堆叠构建多层感知机，从而得到表达能力更强的模型的方法。在本实训中，我们更进一步，来学习神经网络的结构。神经网络由若干神经网络层堆叠而成，不同的层能够有效的提取数据的特征，实现对输入数据从低阶特征到高阶特征的逐渐提取，并完成分类、回归等机器学习任务。多层感知机其实已经是一种非常简单的神经网络。常见的神经网络包括多层感知机、卷积神经网络、循环神经网络等。下图展示了一个简单的神经网络模型。



图1
图1 简单神经网络
神经网络在实现上包括前向传播和反向传播两个部分。前向传播在训练和预测时都会用到，用于计算网络模型的预测结果和损失函数。而反向传播则在训练过程中，用来计算参数的梯度，使用梯度下降法来对网络模型进行训练。在下面的几个实训中，我们将关注神经网络模型的前向传播，而将反向传播放在后面的实训介绍。

从本实训开始，你将会使用numpy来实现一个简单的深度学习框架，包括常见的神经网络层的前向和反向传播，以及神经网络训练的基本方法。

全连接层的定义
回顾之前使用感知机实现逻辑门的例子，每个输出都是所有输入信号的线性组合。把每个输入和输出信号都看作是一个神经元，那么输出神经元和输入神经元之间是两两互相连接的，这种网络层叫做全连接层。

形式化地，一个包含N个输入神经元，M个输出神经元的全连接层包含两组参数：权重W∈R 
N×M
 和偏置b∈R 
M
 ，其输入可以看作是一个N维的（列）向量x∈R 
N
 ，此时全连接层的计算可以表示为：

y=x 
T
 W+b

通过全连接层，可以将输入特征进行线性变换，得到一组新的特征。

全连接层的实现
实训已经预先定义了一个FullyConnected类，在该类的构造函数中，其接受对应的权重W和偏置b。权重W是一个N×M的numpy.ndarray，偏置b是一个长度为M的numpy.ndarray，其中N是全连接层的输入通道数，M是全连接层的输出通道数。

在本实训中，你需要实现前向传播函数forward()。forward()函数的输入x是一个维度大于等于2的numpy.ndarray，形状为(B,dim 
1
​
 ,dim 
2
​
 ,...,dim 
k
​
 )，其中B是 batch size，即数据的个数。首先，你需要对x的形状进行调整，将其转化为形状为(B,N)的2维numpy.ndarray。同时还需要记录x的原始形状，并记录在original_x_shape中。最后返回线性变换后的结果。

编程要求
根据提示，在右侧编辑器中 Begin 和 End 之间补充代码，实现上述全连接层的前向传播。

测试说明
平台会对你编写的代码进行测试，测试方法为：平台会随机产生输入x、权重W和偏置b，然后根据你的实现代码，创建一个FullyConnected类的实例，然后利用该实例进行前向传播计算。你的答案将并与标准答案进行比较。因为浮点数的计算可能会有误差，因此只要你的答案与标准答案之间的误差不超过 1e-5 即可。

样例输入：

W: 
[[0.1, 0.2, 0.3],
[0.4, 0.5, 0.6]]
b:
[0.1, 0.2, 0.3]
x:
[[1, 2],
[3, 4]]
则对应的输出神经元为：

[[1.0, 1.4, 1.8],
[2.0, 2.8, 3.6]]
开始你的任务吧，祝你成功！

说点什么
1
resize-icon
12345678910111213141516171819202122
import numpy as np


class FullyConnected:
    def __init__(self, W, b):
        r'''
        全连接层的初始化。

        Parameter:
        - W: numpy.array, (D_in, D_out)

测试结果
测试集1
本关最大执行时间：20秒
下一关
run
评测
