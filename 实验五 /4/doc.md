
夏炘航
gold13260
5.2 Adaboost算法
实验总用时：00:00:03
nav
第1关：Boosting
20
学习内容
参考答案
记录
评论
任务描述
相关知识
什么是集成学习
Boosting
编程要求
测试说明
任务描述
本关任务：根据本节课所学知识完成本关所设置的选择题。

相关知识
为了完成本关任务，你需要掌握：1.什么是集成学习，2.Boosting。

什么是集成学习
集成学习方法是一种常用的机器学习方法，分为 bagging 与 boosting 两种方法，应用十分广泛。集成学习基本思想是：对于一个复杂的学习任务，我们首先构造多个简单的学习模型，然后再把这些简单模型组合成一个高效的学习模型。实际上，就是“三个臭皮匠顶个诸葛亮”的道理。

 

集成学习采取投票的方式来综合多个简单模型的结果，按 bagging 投票思想，如下面例子：

 

假设一共训练了 5 个简单模型，每个模型对分类结果预测如上图，则最终预测结果为：
 A:2 
 B:3 
 3>2 
结果为 B

不过在有的时候，每个模型对分类结果的确定性不一样，即有的对分类结果非常肯定，有的不是很肯定,说明每个模型投的一票应该是有相应的权重来衡量这一票的重要性。就像在歌手比赛中，每个观众投的票记 1 分，而专家投票记 10 分。按 boosting 投票思想，如下例：

 

A：(0.9+0.4+0.3+0.8+0.2)/5=0.52
B：(0.1+0.6+0.7+0.2+0.8)/5=0.48
0.52>0.48
结果为 A

Boosting
提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。

历史上， Kearns 和 Valiant 首先提出了强可学习和弱可学习的概念。指出：在 PAC 学习的框架中，一个概念，如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是 Schapire 后来证明强可学习与弱可学习是等价的，也就是说，在 PAC 学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

这样一来，问题便成为，在学习中，如果已经发现了弱学习算法，那么能否将它提升为强学习算法。大家知道，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。

与 bagging 不同， boosting 采用的是一个串行训练的方法。首先，它训练出一个弱分类器，然后在此基础上，再训练出一个稍好点的弱分类器，以此类推，不断的训练出多个弱分类器，最终再将这些分类器相结合，这就是 boosting 的基本思想，流程如下图：

 

可以看出，子模型之间存在强依赖关系，必须串行生成。 boosting 是利用不同模型的相加，构成一个更好的模型，求取模型一般都采用序列化方法，后面的模型依据前面的模型。

编程要求
根据所学完成右侧选择题。

测试说明
略

开始你的任务吧，祝你成功！

说点什么
resize-icon
题目
1、

现在有一份数据，你随机的将数据分成了n份，然后同时训练n个子模型，再将模型最后相结合得到一个强学习器，这属于boosting方法吗？


A、
是


B、
不是


C、
不确定

2、

对于一个二分类问题，假如现在训练了500个子模型，每个模型权重大小一样。若每个子模型正确率为51%，则整体正确率为多少？若把每个子模型正确率提升到60%，则整体正确率为多少？


A、
51%,60%


B、
60%,90%


C、
65.7%,99.99%


D、
65.7%,90%

测试结果
当前状态未提交，点击评测进行提交。提交后才计算成绩，详情如下：

题目1
未提交
题目2
未提交
本关最大执行时间：20秒
下一关
run
评测