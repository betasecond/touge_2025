
夏炘航
gold11060
4 决策树
实验总用时：00:00:01
nav
第5关：基尼系数
200
学习内容
参考答案
记录
评论
任务描述
相关知识
基尼系数
编程要求
测试说明
任务描述
本关任务：根据本关所学知识，完成calcGini函数。

相关知识
为了完成本关任务，你需要掌握：基尼系数。

基尼系数
在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益率来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？当然有！那就是基尼系数！

CART算法使用基尼系数来代替信息增益率，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益与信息增益率是相反的(它们都是越大越好)。

基尼系数的数学定义为如下，其中D表示数据集，p 
k
​
 表示D中第k个类别在D中所占比例。

Gini(D)=1−sum 
k=1
∣y∣
​
 p 
k
2
​
 

从公式可以看出，相比于信息增益和信息增益率，计算起来更加简单。举个例子，还是使用第二关中提到过的数据集，第一列是编号，第二列是性别，第三列是活跃度，第四列是客户是否流失的标签（0表示未流失，1表示流失）。

编号	性别	活跃度	是否流失
1	男	高	0
2	女	中	0
3	男	低	1
4	女	高	0
5	男	高	0
6	男	中	0
7	男	中	1
8	女	中	0
9	女	低	1
10	女	中	0
11	女	高	0
12	男	低	1
13	女	低	1
14	男	高	0
15	男	高	0
从表格可以看出，D中总共有2个类别，设类别为0的比例为p 
1
​
 ，则有p 
1
​
 = 
15
10
​
 。设类别为1的比例为p 
2
​
 ，则有p 
2
​
 = 
15
5
​
 。根据基尼系数的公式可知Gini(D)=1−(p 
1
2
​
 +p 
2
2
​
 )=0.4444。

上面是基于数据集D的基尼系数的计算方法，那么基于数据集D与特征a的基尼系数怎样计算呢？其实和信息增益率的套路差不多。计算公式如下：

Gini(D,a)=sum 
v=1
V
​
  
∣D∣
∣D 
v
 ∣
​
 Gini(D 
v
 )

还是以用户流失的数据为例，现在算一算性别的基尼系数。设性别男为v=1，性别女为v=2则有∣D∣=15，∣D 
1
 ∣=8，∣D 
2
 ∣=7，Gini(D 
1
 )=0.46875，Gini(D 
2
 )=0.40816。所以Gini(D,a)=0.44048。

编程要求
根据提示，在右侧编辑器补充代码，完成calcGini函数实现计算信息增益。

calcGini函数中的参数:

feature：测试用例中字典里的feature，类型为ndarray；

label：测试用例中字典里的label，类型为ndarray；

index：测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算基尼系数。

测试说明
平台会对你编写的代码进行测试，期望您的代码根据输入来输出正确的信息增益，以下为其中一个测试用例：

测试输入：
{'feature':[[0, 1], [1, 0], [1, 2], [0, 0], [1, 1]], 'label':[0, 1, 0, 0, 1], 'index': 0}

预期输出：
0.266667

开始你的任务吧，祝你成功！

说点什么
resize-icon
1234567891011121314
import numpy as np

def calcGini(feature, label, index):
    '''
    计算基尼系数
    :param feature:测试用例中字典里的feature，类型为ndarray
    :param label:测试用例中字典里的label，类型为ndarray
    :param index:测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。
    :return:基尼系数，类型float

测试结果
测试集1
测试集2
测试集3
测试集4
本关最大执行时间：120秒
上一关
下一关
run
评测
