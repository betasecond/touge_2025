任务描述
相关知识
更进一步了解K近邻算法
了解sklearn中KNeighborsClassifier的参数
编程要求
测试说明
任务描述
本关任务：编写一个能对数据进行分类的程序。

相关知识
为了完成本关任务，你需要：

更进一步了解kNN算法；

了解sklearn中KNeighborsClassifier的参数。

更进一步了解K近邻算法
在上一节的内容中，我们已经介绍了kNN算法的基础知识，在这一节我们再介绍一下kNN算法的更多内容。

在kNN算法中，待分析样本的类别是由离其最近的K个样本的类别来决定的。所以kNN算法所考虑到的历史数据信息是很少的，基本只由K值的选择以及距离函数的选择来决定。当K值比较大时，所能考虑到的样本数目会更多，但是kNN算法的初衷，“近朱者赤，近墨者黑”的基本思想就无法得到运用了。而当K值比较小时，所能考虑到的样本数量就很少，这时kNN算法在噪音比较多的数据里效果很差。

除了K值之外，kNN算法的另一个核心参数是距离函数的选择。虽然在上一个实训的描述中，我们是用图片来举例说明kNN算法的。但实际上这里所说的距离与我们日常生活中所意识到的距离是不同的。在日常生活中我们所说的距离往往是欧氏距离，也即平面上两点相连后线段的长度。

欧氏距离的定义如下：

p(A,B)=
i=0
∑
​
(A
i
​
−B
i
​
)
2

​


除此之外，在机器学习中常见的距离定义有以下几种：

汉明距离：两个字符串对应位置不一样的个数。汉明距离是以理查德·卫斯里·汉明的名字命名的。在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数；

马氏距离：表示数据的协方差距离。计算两个样本集相似度的距离；

余弦距离：两个向量的夹角作为一种判别距离的度量；

曼哈顿距离：两点投影到各轴上的距离总和；

切比雪夫距离：两点投影到各轴上距离的最大值；

标准化欧氏距离： 欧氏距离里每一项除以标准差。

还有一种距离叫闵可夫斯基距离，如下：

d(A,B)=(
i=0
∑
​
(A
i
​
−B
i
​
)
p
)
q
1
​



当q为1时，即为曼哈顿距离。当q为2时，即为欧氏距离。

虽然一下子介绍了很多，但大家肯定还是觉得不明就里，但是不用着急，距离的定义在机器学习中是一个核心概念，在之后的学习中还会经常遇到它。在这里介绍距离的目的一个是为了让大家使用k近邻算法时，如果发现效果不太好时，可以通过使用不同的距离定义来尝试改进算法的性能。

了解sklearn中KNeighborsClassifier的参数
想要使用sklearn中使用kNN算法，只需要如下的代码(其中train_feature、train_label和test_feature分别表示训练集数据、训练集标签和测试集数据)：

from sklearn.neighbors import KNeighborsClassifier
clf=KNeighborsClassifier() #生成K近邻分类器
clf.fit(train_feature, train_label)               #训练分类器
predict_result=clf.predict(test_feature)           #进行预测
当我们的kNN算法需要不同的参数时，上面的代码就不能满足我的需要了。所需要做的改变是在
clf=KNeighborsClassifier()这一行中。KNeighborsClassifier()的构造函数其实还是有其他参数的。

比较常用的参数有以下几个:

n_neighbors，即K近邻算法中的K值，为一整数，默认为5；

metric，距离函数。参数可以为字符串（预设好的距离函数）或者是callable（可调用对象，大家不明白的可以理解为函数即可）。默认值为闵可夫斯基距离；

p，当metric为闵可夫斯基距离公式时，上文中的q值，默认为2。

编程要求
请仔细阅读右侧代码，根据方法内的提示，在Begin - End区域内进行代码补充，具体任务如下：

完成classification函数。函数需要完成的功能是使用KNeighborsClassifier对test_feature进行分类。其中函数的参数如下：

train_feature: 训练集数据；

train_label: 训练集标签；

test_feature: 测试集数据。

测试说明
补充完代码后，点击测评，平台会对你编写的代码进行测试，当你的结果与预期输出一致时，即为通过。

平台会对你返回的预测结果来计算准确率，你只需完成classification函数即可。准确率高于0.75视为过关。

预期输出：你的准确率高于0.75

开始你的任务吧，祝你成功！